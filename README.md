V-Eval is a human-centric benchmark suite specifically designed to evaluate the general capabilities of foundational models with a focus on the Vietnamese language. This benchmark covers 61 subjects spanning four categories: STEM, Humanities, Social Sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and tests both general knowledge and problem-solving ability. Please visit our [website](https://veval.ai) for more details. 

We hope V-Eval could help developers track the progress and analyze the important strengths/shortcomings of their models.

## Table of Contents

- [Leaderboard](#leaderboard)
- [Data](#data)
- [How to Evaluate on V-Eval](#how-to-evaluate-on-V-Eval)
- [TODO](#todo)
- [Licenses](#licenses)



## Leaderboard

Below are zero-shot and five-shot accuracies from the models that we evaluate in the initial release, please visit our official.

#### Zero-shot
| Model               | STEM | Social Science | Humanities | Other | Average |
| ------------------- | :--: | :------------: | :--------: | :---: | :-----: |


#### Five-shot
| Model               | STEM | Social Science | Humanities | Other | Average |
| ------------------- | :--: | :------------: | :--------: | :---: | :-----: |


## Data

#### Download

- Method 1: Download the zip file (you can also simply open the following link with the browser):
  ```
  TBU
  ```

- Method 2: Directly load the dataset using [TBU]:

  ```
  TBU
  ```


## How to Evaluate on V-Eval
TBU


## TODO
TBU


## Licenses
TBU
